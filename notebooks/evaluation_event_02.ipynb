{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53nBQpYb37bw"
   },
   "source": [
    "## 1. Data Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 12393,
     "status": "ok",
     "timestamp": 1632683772579,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "qsEynPSV4FKf",
    "outputId": "b0a602e6-abbd-497a-ad9d-abbac38a65ab"
   },
   "outputs": [],
   "source": [
    "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
    "from rdflib.term import URIRef, Literal\n",
    "import csv\n",
    "import rdflib\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'jupyterlab+svg'\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from speakeasypy import Speakeasy, Chatroom\n",
    "from typing import List\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "from thefuzz import fuzz,process\n",
    "import editdistance\n",
    "import itertools\n",
    "\n",
    "import jsonpickle\n",
    "# NOTE: You might have to download a few things for nltk to work properly\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import Tree\n",
    "\n",
    "# NOTE: You might have to download the en_core_web_sm model for this to work\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/melihserin/Desktop/ATAI/uzh__advanced_topics_in_ai'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139148,
     "status": "ok",
     "timestamp": 1632681991550,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "lJqbjO9D4TcN",
    "outputId": "49fe0321-0ef5-4324-d56a-8aba54913547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nbe04587ea1ff42659a60b5bc277710fd (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse('data/14_graph.nt', format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embeddings\n",
    "entity_emb = np.load('data/ddis-graph-embeddings/entity_embeds.npy')\n",
    "relation_emb = np.load('data/ddis-graph-embeddings/relation_embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionaries\n",
    "with open('data/ddis-graph-embeddings/entity_ids.del', 'r') as ifile:\n",
    "    ent2id = {str(rdflib.term.URIRef(ent)): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2ent = {v: k for k, v in ent2id.items()}\n",
    "with open('data/ddis-graph-embeddings/relation_ids.del', 'r') as ifile:\n",
    "    rel2id = {str(rdflib.term.URIRef(rel)): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2rel = {v: k for k, v in rel2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2lbl = {str(ent): str(lbl) for ent, lbl in g.subject_objects(RDFS.label)}\n",
    "lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1632682133729,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "7KQagm9qcEFN"
   },
   "outputs": [],
   "source": [
    "# prefixes used in the graph\n",
    "WD = Namespace('http://www.wikidata.org/entity/')\n",
    "WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
    "SCHEMA = Namespace('http://schema.org/')\n",
    "DDIS = Namespace('http://ddis.ch/atai/')\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Agent Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2203fcc776148cfa8918f34221657cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e303d50995a942d49ed739b1f4dd8e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584cd6174ae74eddb35e35936ff71423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf22943c40a458881ccc864834888d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edd3c354457452e8d8ec33087f1a363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0b6914f6d84119ac9ae17b7f730c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(g):\n",
    "    nodes = {}\n",
    "    query =\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "    SELECT ?lbl WHERE {{\n",
    "        <{}> rdfs:label ?lbl .\n",
    "        FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    graph_entities = set(g.subjects(unique=True)) | {s for s in g.objects(unique=True) if isinstance(s, URIRef)}\n",
    "    for node in graph_entities:\n",
    "        entity = node.toPython()\n",
    "        if isinstance(node, URIRef):            \n",
    "            qres = g.query(query.format(entity))\n",
    "            for row in qres:\n",
    "                answer = row.lbl\n",
    "            \n",
    "            nodes[str(answer)] = entity\n",
    "    return nodes\n",
    "\n",
    "def extract_predicates(g):\n",
    "    query =\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "    SELECT ?lbl WHERE {{\n",
    "        <{}> rdfs:label ?lbl .\n",
    "        FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    predicates = {}\n",
    "\n",
    "    graph_predicates = set(g.predicates(unique=True))\n",
    "    for predicate in graph_predicates:\n",
    "        predicate_ = predicate.toPython()       \n",
    "        qres = g.query(query.format(predicate_))\n",
    "        for row in qres:\n",
    "            answer = row.lbl\n",
    "        \n",
    "        predicates[str(answer)] = predicate_\n",
    "\n",
    "    return predicates\n",
    "\n",
    "# make variables for the nodes and predicates path\n",
    "nodes_path = 'data/processed/nodes.json'\n",
    "predicates_path = 'data/processed/predicates.json'\n",
    "\n",
    "# check indiviudally if the files exist and if so load them\n",
    "if os.path.exists(nodes_path):\n",
    "    with open(nodes_path, 'r') as ifile:\n",
    "        nodes = jsonpickle.decode(ifile.read())\n",
    "else:\n",
    "    nodes = extract_nodes(g)\n",
    "    with open(nodes_path, 'w') as ofile:\n",
    "        ofile.write(jsonpickle.encode(nodes))\n",
    "\n",
    "if os.path.exists(predicates_path):\n",
    "    with open(predicates_path, 'r') as ifile:\n",
    "        predicates = jsonpickle.decode(ifile.read())\n",
    "else:\n",
    "    predicates = extract_predicates(g)\n",
    "    with open(predicates_path, 'w') as ofile:\n",
    "        ofile.write(jsonpickle.encode(predicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ner = ner\n",
    "        self.factual_question_patterns = [\n",
    "            \"who is the (.*) of ENTITY\",\n",
    "            \"what is the (.*) of ENTITY\",\n",
    "            \"who (.*) ENTITY\",\n",
    "            \"when was ENTITY (.*)\",\n",
    "            \"where was ENTITY (.*)\",\n",
    "            \"where is ENTITY (.*)\"\n",
    "        ]\n",
    "        self.nodes = nodes\n",
    "        self.predicates = predicates\n",
    "        self.entity_emb = entity_emb\n",
    "        self.relation_emb = relation_emb\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.ent2lbl = ent2lbl\n",
    "        self.lbl2ent = lbl2ent\n",
    "        self.id2ent = id2ent\n",
    "\n",
    "    def entity_extraction(self,ner_results,example):\n",
    "        entity = \"\"\n",
    "        entity_list = []\n",
    "        reset=0\n",
    "        for entity_num in range(len(ner_results)):\n",
    "            if (ner_results[entity_num][\"word\"].find(\"#\") ==-1) & (reset!=0):\n",
    "                entity = entity + \" \" + ner_results[entity_num][\"word\"]\n",
    "                reset +=1\n",
    "            else:\n",
    "                entity = entity + ner_results[entity_num][\"word\"].replace(\"#\",\"\")\n",
    "                reset +=1\n",
    "            \n",
    "            if (entity_num < len(ner_results)-1):\n",
    "                if (ner_results[entity_num+1][\"start\"] - ner_results[entity_num][\"end\"] > 3):\n",
    "                    entity_list.append(entity)\n",
    "                    reset=0\n",
    "                    entity = \"\"\n",
    "                    continue\n",
    "            else:\n",
    "                entity_list.append(entity)\n",
    "                reset=0\n",
    "                continue\n",
    "        for n,entity in enumerate(entity_list):\n",
    "            if len(entity.split(\" \"))>1:\n",
    "                first_word = entity.split(\" \")[0]\n",
    "                last_word = entity.split(\" \")[-1]\n",
    "                search_str = first_word + \"(.+?)\" + last_word\n",
    "                entity_list[n] = re.search(search_str,example).group(0)\n",
    "            else:\n",
    "                continue\n",
    "        return entity_list\n",
    "    \n",
    "    def preprocessing_before_ner(self,question):\n",
    "        try:\n",
    "            question_new=re.sub(re.search(\"(.*?)of\",question).group(0), re.search(\"(.*?)of\",question).group(0).lower() ,question)\n",
    "        except:\n",
    "            question_new=question\n",
    "        return question_new\n",
    "    \n",
    "    def preprocessing(self,question):\n",
    "        return question.replace(\"?\",\"\").lower()\n",
    "\n",
    "    # which pattern is used in the given question?\n",
    "    def pattern_detection(self,ner_results,example):\n",
    "        entities_extracted = self.entity_extraction(ner_results,example)\n",
    "\n",
    "        pattern_and_entity = [[re.sub(\"ENTITY\",entity_from_list, pattern),entity_from_list] for pattern in self.factual_question_patterns for entity_from_list in entities_extracted]\n",
    "        pattern_entity_included = [lists[0] for lists in pattern_and_entity]\n",
    "        entity_from_pattern_and_entity = list(dict.fromkeys([lists[1] for lists in pattern_and_entity]))\n",
    "\n",
    "\n",
    "        question_pattern = process.extract(example,pattern_entity_included,scorer=fuzz.ratio)[0][0]\n",
    "        question_pattern_ = [re.sub(value,\"ENTITY\",question_pattern) for value in entity_from_pattern_and_entity if question_pattern.find(value)!=-1][0]\n",
    "\n",
    "        index = [num for num,value in enumerate(self.factual_question_patterns) if value==question_pattern_][0]\n",
    "\n",
    "        return question_pattern,index\n",
    "\n",
    "    def relation_extraction(self,ner_results,example):\n",
    "        question_pattern, index = self.pattern_detection(ner_results,example)\n",
    "        relation = re.match(self.preprocessing(question_pattern), self.preprocessing(example)).group(1)\n",
    "\n",
    "        return relation # take care of directed, released, etc. cases\n",
    "    \n",
    "    def match_things(self,dict, input):\n",
    "        tmp = 9999\n",
    "        match_key = \"\"\n",
    "        match_value = \"\"\n",
    "        for key, value in dict.items():\n",
    "            if editdistance.eval(key.lower(), input) < tmp:\n",
    "                tmp = editdistance.eval(key.lower(), input)\n",
    "                match_key = key\n",
    "                match_value = value\n",
    "        \n",
    "        return match_key,match_value\n",
    "    \n",
    "    def final_query(self,matched_entity,matched_entity_url,matched_predicate,matched_predicate_url):\n",
    "        query_option1 =\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "        SELECT ?lbl WHERE {{\n",
    "            <{}> <{}> ?answer.\n",
    "            ?answer rdfs:label ?lbl .\n",
    "            FILTER(LANG(?lbl) = \"en\").\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\".format(matched_entity_url,matched_predicate_url)\n",
    "\n",
    "        query_option2 =\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "        SELECT ?lbl WHERE {{\n",
    "            ?answer <{}> <{}>.\n",
    "            ?answer rdfs:label ?lbl .\n",
    "            FILTER(LANG(?lbl) = \"en\").\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\".format(matched_predicate_url,matched_entity_url)\n",
    "\n",
    "        qres1 = g.query(query_option1)\n",
    "        qres2 = g.query(query_option2)\n",
    "\n",
    "        answer = \"\"\n",
    "        try:\n",
    "            for row in qres1:\n",
    "                answer = row.lbl\n",
    "        except answer == \"\":\n",
    "            for row in qres2:\n",
    "                answer = row.lbl \n",
    "\n",
    "\n",
    "        if answer == \"\":\n",
    "            answer1, answer2, answer3 = self.final_embed(matched_entity_url,matched_predicate_url)    \n",
    "            return f\"According to the embeddings, the {matched_predicate} of {matched_entity} is {answer1}, {answer2}, {answer3}.\"      \n",
    "        else:\n",
    "            answer1, answer2, answer3 = self.final_embed(matched_entity_url,matched_predicate_url)    \n",
    "            return f\"\"\"According to the embeddings, the {matched_predicate} of {matched_entity} is {answer1}, {answer2}, or {answer3}. On the other hand, as of my knowledge, the {matched_predicate} of {matched_entity} is {answer}.\"\"\"\n",
    "    \n",
    "    def final_embed(self,matched_entity_url,matched_predicate_url):\n",
    "        head = self.entity_emb[self.ent2id[matched_entity_url]]\n",
    "        pred = self.relation_emb[self.rel2id[matched_predicate_url]]\n",
    "        # add vectors according to TransE scoring function.\n",
    "        lhs = head + pred\n",
    "        # compute distance to *any* entity\n",
    "        dist = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        # find most plausible entities\n",
    "        most_likely = dist.argsort()\n",
    "        # compute ranks of entities\n",
    "        ranks = dist.argsort().argsort()\n",
    "\n",
    "        most_plausible_3_answers = [(str(self.id2ent[idx]), self.ent2lbl[self.id2ent[idx]])\n",
    "            for rank, idx in enumerate(most_likely[:3])]\n",
    "        \n",
    "        answer1, answer2, answer3 = most_plausible_3_answers[0][1],most_plausible_3_answers[1][1],most_plausible_3_answers[2][1]\n",
    "        return answer1, answer2, answer3\n",
    "\n",
    "    def forward(self,input):\n",
    "        ner_results = self.ner(self.preprocessing_before_ner(input))\n",
    "        entities = self.entity_extraction(ner_results,input)\n",
    "        entity = entities[0]\n",
    "        relation = self.relation_extraction(ner_results,input)\n",
    "\n",
    "        matched_entity, matched_entity_url= self.match_things(self.nodes, entity)\n",
    "        matched_predicate, matched_predicate_url= self.match_things(self.predicates, relation)\n",
    "\n",
    "        output = self.final_query(matched_entity,matched_entity_url,matched_predicate,matched_predicate_url)\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the embeddings, the genre of Good Neighbors is drama, comedy-drama, or comedy film. On the other hand, as of my knowledge, the genre of Good Neighbors is art film.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = Chatbot()\n",
    "chatbot(\"What is the genre of Good Neighbors?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out different parsings with nltk / spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'the', 'genre', 'of', 'Good', 'Neighbors', '?']\n",
      "[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('genre', 'NN'), ('of', 'IN'), ('Good', 'JJ'), ('Neighbors', 'NNS'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(\"What is the genre of Good Neighbors?\")\n",
    "print(tokens)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  What/WP\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  genre/NN\n",
      "  of/IN\n",
      "  (GPE Good/JJ)\n",
      "  Neighbors/NNS\n",
      "  ?/.)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)\n",
    "\n",
    "entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[what is the genre of Good Neighbors?]\n",
      "      is                    \n",
      "  ____|________              \n",
      " |    |      genre          \n",
      " |    |    ____|_______      \n",
      " |    |   |            of   \n",
      " |    |   |            |     \n",
      " |    |   |        Neighbors\n",
      " |    |   |            |     \n",
      "what  ?  the          Good  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('what is the genre of Good Neighbors?')\n",
    "doc\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "print(list(doc.sents))\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6de2f8972dfc4c2aac901329a22c476f-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">what</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">genre</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Good</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Neighbors?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6de2f8972dfc4c2aac901329a22c476f-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6de2f8972dfc4c2aac901329a22c476f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6de2f8972dfc4c2aac901329a22c476f-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6de2f8972dfc4c2aac901329a22c476f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6de2f8972dfc4c2aac901329a22c476f-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6de2f8972dfc4c2aac901329a22c476f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6de2f8972dfc4c2aac901329a22c476f-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6de2f8972dfc4c2aac901329a22c476f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6de2f8972dfc4c2aac901329a22c476f-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6de2f8972dfc4c2aac901329a22c476f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6de2f8972dfc4c2aac901329a22c476f-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6de2f8972dfc4c2aac901329a22c476f-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('what is the genre of Good Neighbors?')\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5e46a679e40b4052be97013dac864566-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Who</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">director</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Good</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Neighbors?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e46a679e40b4052be97013dac864566-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e46a679e40b4052be97013dac864566-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e46a679e40b4052be97013dac864566-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e46a679e40b4052be97013dac864566-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e46a679e40b4052be97013dac864566-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e46a679e40b4052be97013dac864566-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e46a679e40b4052be97013dac864566-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e46a679e40b4052be97013dac864566-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e46a679e40b4052be97013dac864566-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e46a679e40b4052be97013dac864566-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e46a679e40b4052be97013dac864566-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e46a679e40b4052be97013dac864566-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('Who is the director of Good Neighbors?')\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"54cbf885d5a34085a9ed68a3024e6804-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Who</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">directed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Good</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Neighbors?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-54cbf885d5a34085a9ed68a3024e6804-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-54cbf885d5a34085a9ed68a3024e6804-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-54cbf885d5a34085a9ed68a3024e6804-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-54cbf885d5a34085a9ed68a3024e6804-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-54cbf885d5a34085a9ed68a3024e6804-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-54cbf885d5a34085a9ed68a3024e6804-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-54cbf885d5a34085a9ed68a3024e6804-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-54cbf885d5a34085a9ed68a3024e6804-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('Who directed of Good Neighbors?')\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse all nouns in the phrase including compounds\n",
    "# also check for compounds and add those\n",
    "\n",
    "def extract_entities(doc):\n",
    "    sent = list(doc.sents)[0]\n",
    "\n",
    "    root_type = sent.root.pos_\n",
    "\n",
    "    entity1 = ''\n",
    "    entity2 = ''\n",
    "\n",
    "    # Case \"Who is the director of Good Neighbors?\"\n",
    "    if root_type == 'AUX':\n",
    "        for child in sent.root.children:\n",
    "            if child.dep_ == 'nsubj':\n",
    "                entity1 = child.text\n",
    "\n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ == 'compound':\n",
    "                        entity1 = subchild.text + ' ' + entity1\n",
    "\n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ == 'prep':\n",
    "                        for subsubchild in subchild.children:\n",
    "                            if subsubchild.dep_ == 'pobj':\n",
    "                                entity2 = subsubchild.text\n",
    "\n",
    "                                for subsubsubchild in subsubchild.children:\n",
    "                                    if subsubsubchild.dep_ == 'compound':\n",
    "                                        entity2 = subsubsubchild.text + ' ' + entity2\n",
    "\n",
    "        return [entity1, entity2]\n",
    "                \n",
    "    # Case \"Who directed Good Neighbors?\"\n",
    "    elif root_type == 'VERB':\n",
    "        entity1 = sent.root.text\n",
    "\n",
    "        for child in sent.root.children:\n",
    "            if child.dep_ == 'dobj':\n",
    "                entity2 = child.text\n",
    "\n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ == 'compound':\n",
    "                        entity2 = subchild.text + ' ' + entity2\n",
    "\n",
    "        return [entity1, entity2]\n",
    "\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def parse_query(query):\n",
    "    doc = nlp(query)\n",
    "    entities = extract_entities(doc)\n",
    "    print(query)\n",
    "    print(f\"\\t{entities[0]} -> {entities[1]}\")\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the genre of Good Neighbors?\n",
      "\tgenre -> Good Neighbors\n",
      "Who is the director of Good Neighbors?\n",
      "\tdirector -> Good Neighbors\n",
      "Who directed Good Neighbors?\n",
      "\tdirected -> Good Neighbors\n"
     ]
    }
   ],
   "source": [
    "parse_query('What is the genre of Good Neighbors?')\n",
    "parse_query('Who is the director of Good Neighbors?')\n",
    "parse_query('Who directed Good Neighbors?')\n",
    "\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WN_NOUN = 'n'\n",
    "WN_VERB = 'v'\n",
    "WN_ADJECTIVE = 'a'\n",
    "WN_ADJECTIVE_SATELLITE = 's'\n",
    "WN_ADVERB = 'r'\n",
    "\n",
    "\n",
    "def convert(input, from_pos, to_pos):    \n",
    "    \"\"\" Transform words given from/to POS tags \"\"\"\n",
    "    words,temp_word_list=[],[]\n",
    "    for index,word in enumerate(input.split(\" \")):\n",
    "        synsets = wn.synsets(word, pos=from_pos)\n",
    "\n",
    "        # Word not found\n",
    "        if not synsets:\n",
    "            if len(words)==0:\n",
    "                words.append((word,1.0))\n",
    "            else:\n",
    "                words =[(w+\" \"+word, p) for w,p in words]\n",
    "        else:\n",
    "            # Get all lemmas of the word (consider 'a'and 's' equivalent)\n",
    "            lemmas = []\n",
    "            for s in synsets:\n",
    "                for l in s.lemmas():\n",
    "                    if s.name().split('.')[1] == from_pos or from_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and s.name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "                        lemmas += [l]\n",
    "\n",
    "            # Get related forms\n",
    "            derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
    "            # filter only the desired pos (consider 'a' and 's' equivalent)\n",
    "            related_noun_lemmas = []\n",
    "\n",
    "            for drf in derivationally_related_forms:\n",
    "                if from_pos == \"n\":\n",
    "                    related_noun_lemmas += [drf[0]]\n",
    "                else:\n",
    "                    for l in drf[1]:\n",
    "                        if l.synset().name().split('.')[1] == to_pos or to_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and l.synset().name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "                            related_noun_lemmas += [l]\n",
    "\n",
    "            # Extract the words from the lemmas\n",
    "            temp_word_list=[l.name() for l in related_noun_lemmas]\n",
    "            temp_word_list = [(w, float(temp_word_list.count(w)) / len(temp_word_list)) for w in set(temp_word_list)]\n",
    "\n",
    "            # Take all the combinations for synonyms of different words\n",
    "            # Build the result in the form of a list containing tuples (word, probability)\n",
    "            if len(words)==0:\n",
    "                words=temp_word_list\n",
    "            else:\n",
    "                words =[(w_b+\" \"+w_t, p_b*p_t) for w_b,p_b in words for w_t,p_t in temp_word_list]\n",
    "                words.sort(key=lambda w:-w[1])\n",
    "\n",
    "    # return all the possibilities sorted by probability\n",
    "    return words\n",
    "\n",
    "convert('MPAA film rating', WN_NOUN, WN_NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guide', 0.075),\n",
       " ('organisation', 0.075),\n",
       " ('director', 0.075),\n",
       " ('head', 0.075),\n",
       " ('organization', 0.075),\n",
       " ('address', 0.05),\n",
       " ('target', 0.05),\n",
       " ('aim', 0.05),\n",
       " ('steering', 0.05),\n",
       " ('place', 0.025),\n",
       " ('leader', 0.025),\n",
       " ('lead', 0.025),\n",
       " ('sending', 0.025),\n",
       " ('directive', 0.025),\n",
       " ('mastermind', 0.025),\n",
       " ('engineer', 0.025),\n",
       " ('steerage', 0.025),\n",
       " ('orchestration', 0.025),\n",
       " ('organizer', 0.025),\n",
       " ('addressee', 0.025),\n",
       " ('maneuverer', 0.025),\n",
       " ('conducting', 0.025),\n",
       " ('heading', 0.025),\n",
       " ('manoeuvre', 0.025),\n",
       " ('steerer', 0.025),\n",
       " ('channelisation', 0.025)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OLD VERSION\n",
    "\n",
    "# WN_NOUN = 'n'\n",
    "# WN_VERB = 'v'\n",
    "# WN_ADJECTIVE = 'a'\n",
    "# WN_ADJECTIVE_SATELLITE = 's'\n",
    "# WN_ADVERB = 'r'\n",
    "\n",
    "\n",
    "# def convert(word, from_pos, to_pos):    \n",
    "#     \"\"\" Transform words given from/to POS tags \"\"\"\n",
    "\n",
    "#     synsets = wn.synsets(word, pos=from_pos)\n",
    "\n",
    "#     # Word not found\n",
    "#     if not synsets:\n",
    "#         return []\n",
    "\n",
    "#     # Get all lemmas of the word (consider 'a'and 's' equivalent)\n",
    "#     lemmas = []\n",
    "#     for s in synsets:\n",
    "#         for l in s.lemmas():\n",
    "#             if s.name().split('.')[1] == from_pos or from_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and s.name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "#                 lemmas += [l]\n",
    "\n",
    "#     # Get related forms\n",
    "#     derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
    "\n",
    "#     # filter only the desired pos (consider 'a' and 's' equivalent)\n",
    "#     related_noun_lemmas = []\n",
    "\n",
    "#     for drf in derivationally_related_forms:\n",
    "#         for l in drf[1]:\n",
    "#             if l.synset().name().split('.')[1] == to_pos or to_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and l.synset().name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "#                 related_noun_lemmas += [l]\n",
    "\n",
    "#     # Extract the words from the lemmas\n",
    "#     words = [l.name() for l in related_noun_lemmas]\n",
    "#     len_words = len(words)\n",
    "\n",
    "#     # Build the result in the form of a list containing tuples (word, probability)\n",
    "#     result = [(w, float(words.count(w)) / len_words) for w in set(words)]\n",
    "#     result.sort(key=lambda w:-w[1])\n",
    "\n",
    "#     # return all the possibilities sorted by probability\n",
    "#     return result\n",
    "\n",
    "# convert('directed', WN_VERB, WN_NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "director\n"
     ]
    }
   ],
   "source": [
    "chosen_option = None\n",
    "distance = np.inf\n",
    "\n",
    "#for option in convert('directed', WN_VERB, WN_NOUN):\n",
    "for option in predicates.keys():\n",
    "    candidate = option\n",
    "    candidate_distance = editdistance.eval(candidate, 'directed')\n",
    "\n",
    "    if candidate_distance < distance:\n",
    "        chosen_option = option\n",
    "        distance = candidate_distance\n",
    "\n",
    "print(chosen_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "listen_freq = 2\n",
    "chatbot = Chatbot()\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        # Initialize the Speakeasy Python framework and login.\n",
    "        self.speakeasy = Speakeasy(host=config(\"UZH_SPEAKEASY_HOST\"), username=username, password=password)\n",
    "        self.speakeasy.login()  # This framework will help you log out automatically when the program terminates.\n",
    "\n",
    "    def listen(self):\n",
    "        while True:\n",
    "            # only check active chatrooms (i.e., remaining_time > 0) if active=True.\n",
    "            rooms: List[Chatroom] = self.speakeasy.get_rooms(active=True)\n",
    "            for room in rooms:\n",
    "                if not room.initiated:\n",
    "                    # send a welcome message if room is not initiated\n",
    "                    room.post_messages(f'Hello! And Grüetzi, Merhaba, Bonjour! How can I help you today?')\n",
    "                    room.initiated = True\n",
    "                # Retrieve messages from this chat room.\n",
    "                # If only_partner=True, it filters out messages sent by the current bot.\n",
    "                # If only_new=True, it filters out messages that have already been marked as processed.\n",
    "                for message in room.get_messages(only_partner=True, only_new=True):\n",
    "                    print(\n",
    "                        f\"\\t- Chatroom {room.room_id} \"\n",
    "                        f\"- new message #{message.ordinal}: '{message.message}' \"\n",
    "                        f\"- {self.get_time()}\")\n",
    "\n",
    "                    # Implement your agent here #\n",
    "                    if (message.message.lower() == \"hi\") | (message.message.lower() == \"hello\"):\n",
    "                        answer='Hello! And Grüetzi, Merhaba, Bonjour! How can I help you today?'\n",
    "                    else:\n",
    "                        try:\n",
    "                            answer = chatbot(message.message)\n",
    "                        except:\n",
    "                            answer = \"Sorry :( I could not understand you. Can you rephrase your question?\"\n",
    "                    # Send a message to the corresponding chat room using the post_messages method of the room object.\n",
    "                    room.post_messages(f\"{answer}\")\n",
    "                    # Mark the message as processed, so it will be filtered out when retrieving new messages.\n",
    "                    room.mark_as_processed(message)\n",
    "\n",
    "                # Retrieve reactions from this chat room.\n",
    "                # If only_new=True, it filters out reactions that have already been marked as processed.\n",
    "                for reaction in room.get_reactions(only_new=True):\n",
    "                    print(\n",
    "                        f\"\\t- Chatroom {room.room_id} \"\n",
    "                        f\"- new reaction #{reaction.message_ordinal}: '{reaction.type}' \"\n",
    "                        f\"- {self.get_time()}\")\n",
    "\n",
    "                    # Implement your agent here #\n",
    "\n",
    "                    room.post_messages(f\"Received your reaction: '{reaction.type}' \")\n",
    "                    room.mark_as_processed(reaction)\n",
    "\n",
    "            time.sleep(listen_freq)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time():\n",
    "        return time.strftime(\"%H:%M:%S, %d-%m-%Y\", time.localtime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_bot = Agent(config(\"UZH_BOT_USERNAME\"), config(\"UZH_BOT_PASSWORD\"))\n",
    "demo_bot.listen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicate match using entity's relations in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_things(dict, input):\n",
    "    input_list = input.split(\" \")\n",
    "    tmp = np.inf\n",
    "    tmpp= [len(word) for word in input_list]\n",
    "    match_key = \"\"\n",
    "    match_value = \"\"\n",
    "    for key, value in dict.items():\n",
    "        if editdistance.eval(key, input) < tmp:\n",
    "            tmp = editdistance.eval(key, input)\n",
    "            key_list = key.split(\" \")\n",
    "            len_input_list, len_key = len(input_list), len(key_list)\n",
    "            is_input_list_longer = (len_input_list>len_key)\n",
    "            index_input_list = list(range(len_input_list)) + [len_input_list-1]*(len_key-len_input_list)*(not is_input_list_longer)\n",
    "            index_key = list(range(len_key)) + [len_key-1]*(len_input_list-len_key)*(is_input_list_longer)\n",
    "            word_wise_comparison = [editdistance.eval(key_list[k], input_list[i]) for i,k in zip(index_input_list,index_key)]\n",
    "            if len_input_list == len(word_wise_comparison):\n",
    "                bool_update = [(tmpp[i] > word_wise_comparison[i]) for i in range(len_input_list)]\n",
    "            else:\n",
    "                bool_update = [(tmpp[i] > word_wise_comparison[k]) for i,k in zip(index_input_list,index_key)]\n",
    "            if sum(bool_update)>0:\n",
    "                if len_input_list == len(word_wise_comparison):\n",
    "                    tmpp = word_wise_comparison\n",
    "                else:\n",
    "                    tmpp = word_wise_comparison[:len_input_list]\n",
    "                    tmpp[len_input_list-1] += sum(word_wise_comparison[len_input_list:])\n",
    "                match_key = key\n",
    "                match_value = value   \n",
    "    return match_key,match_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = []\n",
    "is_two_fncs = 1\n",
    "for child in [children for children,score in convert('released', WN_VERB, WN_NOUN)]:\n",
    "    # editdistance and fuzz similarity fnc\n",
    "    matched.append([match_things(predicates, child)[0], process.extract(child,[key for key,value in predicates.items()],scorer=fuzz.ratio)[0][0]])\n",
    "matched = list(matched for matched,_ in itertools.groupby(matched))\n",
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_option = None\n",
    "distance = np.inf\n",
    "# Good Neighbors' relations\n",
    "predicates_of_entity = list( dict.fromkeys([k for s,p,o in g.triples((WD.Q3110682, None, None)) for k,v in predicates.items() if v==str(p)]) )\n",
    "\n",
    "\n",
    "for option in matched:\n",
    "    if isinstance(option,list):\n",
    "        if option[0] in predicates_of_entity:\n",
    "            chosen_option= option[0]\n",
    "            break\n",
    "        elif option[1] in predicates_of_entity:\n",
    "            chosen_option= option[1]\n",
    "            break\n",
    "print(chosen_option)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNC2BzrVbq+mNDgGNziLFpm",
   "collapsed_sections": [],
   "name": "dataset_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
