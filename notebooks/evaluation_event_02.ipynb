{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53nBQpYb37bw"
   },
   "source": [
    "## 1. Data Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 12393,
     "status": "ok",
     "timestamp": 1632683772579,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "qsEynPSV4FKf",
    "outputId": "b0a602e6-abbd-497a-ad9d-abbac38a65ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
    "from rdflib.term import URIRef, Literal\n",
    "import csv\n",
    "import rdflib\n",
    "pio.renderers.default = 'jupyterlab+svg'\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from speakeasypy import Speakeasy, Chatroom\n",
    "from typing import List\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "from thefuzz import fuzz,process\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139148,
     "status": "ok",
     "timestamp": 1632681991550,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "lJqbjO9D4TcN",
    "outputId": "49fe0321-0ef5-4324-d56a-8aba54913547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N17e29349562d44fbbee7daeabdefe2b7 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse('/Users/melihserin/Desktop/ATAI/dataset/14_graph.nt', format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embeddings\n",
    "entity_emb = np.load('/Users/melihserin/Desktop/ATAI/dataset/ddis-graph-embeddings/entity_embeds.npy')\n",
    "relation_emb = np.load('/Users/melihserin/Desktop/ATAI/dataset/ddis-graph-embeddings/relation_embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionaries\n",
    "with open('/Users/melihserin/Desktop/ATAI/dataset/ddis-graph-embeddings/entity_ids.del', 'r') as ifile:\n",
    "    ent2id = {str(rdflib.term.URIRef(ent)): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2ent = {v: k for k, v in ent2id.items()}\n",
    "with open('/Users/melihserin/Desktop/ATAI/dataset/ddis-graph-embeddings/relation_ids.del', 'r') as ifile:\n",
    "    rel2id = {str(rdflib.term.URIRef(rel)): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2rel = {v: k for k, v in rel2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2lbl = {str(ent): str(lbl) for ent, lbl in g.subject_objects(RDFS.label)}\n",
    "lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1632682133729,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "7KQagm9qcEFN"
   },
   "outputs": [],
   "source": [
    "# prefixes used in the graph\n",
    "WD = Namespace('http://www.wikidata.org/entity/')\n",
    "WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
    "SCHEMA = Namespace('http://schema.org/')\n",
    "DDIS = Namespace('http://ddis.ch/atai/')\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Agent Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(g):\n",
    "    nodes = {}\n",
    "    query =\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "    SELECT ?lbl WHERE {{\n",
    "        <{}> rdfs:label ?lbl .\n",
    "        FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    graph_entities = set(str(graph.subjects(unique=True))) | {str(s) for s in graph.objects(unique=True) if isinstance(s, URIRef)}\n",
    "    for node in graph_entities:\n",
    "        entity = node.toPython()\n",
    "        if isinstance(node, URIRef):            \n",
    "            qres = g.query(query.format(entity))\n",
    "            for row in qres:\n",
    "                answer = row.lbl\n",
    "            \n",
    "            nodes[str(answer)] = entity\n",
    "    return nodes\n",
    "\n",
    "def extract_predicates(g):\n",
    "    query =\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "    SELECT ?lbl WHERE {{\n",
    "        <{}> rdfs:label ?lbl .\n",
    "        FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    predicates = {}\n",
    "\n",
    "    graph_predicates = set(g.predicates(unique=True))\n",
    "    for predicate in graph_predicates:\n",
    "        predicate_ = predicate.toPython()       \n",
    "        qres = g.query(query.format(predicate_))\n",
    "        for row in qres:\n",
    "            answer = row.lbl\n",
    "        \n",
    "        predicates[str(answer)] = predicate_\n",
    "\n",
    "    return predicates\n",
    "\n",
    "\n",
    "nodes = extract_nodes(g)\n",
    "predicates = extract_predicates(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ner = nlp\n",
    "        self.factual_question_patterns = [\n",
    "            \"who is the (.*) of ENTITY\",\n",
    "            \"what is the (.*) of ENTITY\",\n",
    "            \"who (.*) ENTITY\",\n",
    "            \"when was ENTITY (.*)\",\n",
    "            \"where was ENTITY (.*)\",\n",
    "            \"where is ENTITY (.*)\"\n",
    "        ]\n",
    "        self.nodes = nodes\n",
    "        self.predicates = predicates\n",
    "        self.entity_emb = entity_emb\n",
    "        self.relation_emb = relation_emb\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.ent2lbl = ent2lbl\n",
    "        self.lbl2ent = lbl2ent\n",
    "        self.id2ent = id2ent\n",
    "\n",
    "    def entity_extraction(self,ner_results,example):\n",
    "        entity = \"\"\n",
    "        entity_list = []\n",
    "        reset=0\n",
    "        for entity_num in range(len(ner_results)):\n",
    "            if (ner_results[entity_num][\"word\"].find(\"#\") ==-1) & (reset!=0):\n",
    "                entity = entity + \" \" + ner_results[entity_num][\"word\"]\n",
    "                reset +=1\n",
    "            else:\n",
    "                entity = entity + ner_results[entity_num][\"word\"].replace(\"#\",\"\")\n",
    "                reset +=1\n",
    "            \n",
    "            if (entity_num < len(ner_results)-1):\n",
    "                if (ner_results[entity_num+1][\"start\"] - ner_results[entity_num][\"end\"] > 3):\n",
    "                    entity_list.append(entity)\n",
    "                    reset=0\n",
    "                    entity = \"\"\n",
    "                    continue\n",
    "            else:\n",
    "                entity_list.append(entity)\n",
    "                reset=0\n",
    "                continue\n",
    "        \n",
    "        for n,entity in enumerate(entity_list):\n",
    "            if len(entity.split(\" \"))>1:\n",
    "                first_word = entity.split(\" \")[0]\n",
    "                last_word = entity.split(\" \")[-1]\n",
    "                search_str = first_word + \"(.+?)\" + last_word\n",
    "                entity_list[n] = re.search(search_str,example).group(0)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return entity_list\n",
    "    \n",
    "    def preprocessing_before_ner(self,question):\n",
    "        try:\n",
    "            question_new=re.sub(re.search(\"(.*?)of\",question).group(0), re.search(\"(.*?)of\",question).group(0).lower() ,question)\n",
    "        except:\n",
    "            question_new=question\n",
    "        return question_new\n",
    "    \n",
    "    def preprocessing(self,question):\n",
    "        return question.replace(\"?\",\"\").lower()\n",
    "\n",
    "    # which pattern is used in the given question?\n",
    "    def pattern_detection(self,ner_results,example):\n",
    "        entities_extracted = self.entity_extraction(ner_results,example)\n",
    "\n",
    "        pattern_and_entity = [[re.sub(\"ENTITY\",entity_from_list, pattern),entity_from_list] for pattern in self.factual_question_patterns for entity_from_list in entities_extracted]\n",
    "        pattern_entity_included = [lists[0] for lists in pattern_and_entity]\n",
    "        entity_from_pattern_and_entity = list(dict.fromkeys([lists[1] for lists in pattern_and_entity]))\n",
    "\n",
    "\n",
    "        question_pattern = process.extract(example,pattern_entity_included,scorer=fuzz.ratio)[0][0]\n",
    "        question_pattern_ = [re.sub(value,\"ENTITY\",question_pattern) for value in entity_from_pattern_and_entity if question_pattern.find(value)!=-1][0]\n",
    "\n",
    "        index = [num for num,value in enumerate(self.factual_question_patterns) if value==question_pattern_][0]\n",
    "\n",
    "        return question_pattern,index\n",
    "\n",
    "    def relation_extraction(self,ner_results,example):\n",
    "        question_pattern, index = self.pattern_detection(ner_results,example)\n",
    "        relation = re.match(self.preprocessing(question_pattern), self.preprocessing(example)).group(1)\n",
    "\n",
    "        return relation # take care of directed, released, etc. cases\n",
    "    \n",
    "    def match_things(self,dict, input):\n",
    "        tmp = 9999\n",
    "        match_key = \"\"\n",
    "        match_value = \"\"\n",
    "        for key, value in dict.items():\n",
    "            if editdistance.eval(key.lower(), input) < tmp:\n",
    "                tmp = editdistance.eval(key.lower(), input)\n",
    "                match_key = key\n",
    "                match_value = value\n",
    "        \n",
    "        return match_key,match_value\n",
    "    \n",
    "    def final_query(self,matched_entity,matched_entity_url,matched_predicate,matched_predicate_url):\n",
    "        query_option1 =\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "        SELECT ?lbl WHERE {{\n",
    "            <{}> <{}> ?answer.\n",
    "            ?answer rdfs:label ?lbl .\n",
    "            FILTER(LANG(?lbl) = \"en\").\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\".format(matched_entity_url,matched_predicate_url)\n",
    "\n",
    "        query_option2 =\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "        SELECT ?lbl WHERE {{\n",
    "            ?answer <{}> <{}>.\n",
    "            ?answer rdfs:label ?lbl .\n",
    "            FILTER(LANG(?lbl) = \"en\").\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\".format(matched_predicate_url,matched_entity_url)\n",
    "\n",
    "        qres1 = g.query(query_option1)\n",
    "        qres2 = g.query(query_option2)\n",
    "\n",
    "        answer = \"\"\n",
    "        try:\n",
    "            for row in qres1:\n",
    "                answer = row.lbl\n",
    "        except answer == \"\":\n",
    "            for row in qres2:\n",
    "                answer = row.lbl \n",
    "\n",
    "\n",
    "        if answer == \"\":\n",
    "            answer1, answer2, answer3 = self.final_embed(matched_entity_url,matched_predicate_url)    \n",
    "            return f\"According to the embeddings, the {matched_predicate} of {matched_entity} is {answer1}, {answer2}, {answer3}.\"      \n",
    "        else:\n",
    "            answer1, answer2, answer3 = self.final_embed(matched_entity_url,matched_predicate_url)    \n",
    "            return f\"\"\"According to the embeddings, the {matched_predicate} of {matched_entity} is {answer1}, {answer2}, or {answer3}. On the other hand, as of my knowledge, the {matched_predicate} of {matched_entity} is {answer}.\"\"\"\n",
    "    \n",
    "    def final_embed(self,matched_entity_url,matched_predicate_url):\n",
    "        head = self.entity_emb[self.ent2id[matched_entity_url]]\n",
    "        pred = self.relation_emb[self.rel2id[matched_predicate_url]]\n",
    "        # add vectors according to TransE scoring function.\n",
    "        lhs = head + pred\n",
    "        # compute distance to *any* entity\n",
    "        dist = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        # find most plausible entities\n",
    "        most_likely = dist.argsort()\n",
    "        # compute ranks of entities\n",
    "        ranks = dist.argsort().argsort()\n",
    "\n",
    "        most_plausible_3_answers = [(str(self.id2ent[idx]), self.ent2lbl[self.id2ent[idx]])\n",
    "            for rank, idx in enumerate(most_likely[:3])]\n",
    "        \n",
    "        answer1, answer2, answer3 = most_plausible_3_answers[0][1],most_plausible_3_answers[1][1],most_plausible_3_answers[2][1]\n",
    "        return answer1, answer2, answer3\n",
    "\n",
    "    def forward(self,input):\n",
    "        ner_results = self.ner(self.preprocessing_before_ner(input))\n",
    "        entities = self.entity_extraction(ner_results,input)\n",
    "        entity = entities[0]\n",
    "        relation = self.relation_extraction(ner_results,input)\n",
    "\n",
    "        matched_entity, matched_entity_url= self.match_things(self.nodes, entity)\n",
    "        matched_predicate, matched_predicate_url= self.match_things(self.predicates, relation)\n",
    "\n",
    "        output = self.final_query(matched_entity,matched_entity_url,matched_predicate,matched_predicate_url)\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the embeddings, the MPAA film rating of Weathering with You is PG-13, PG, or R. On the other hand, as of my knowledge, the MPAA film rating of Weathering with You is NC-17.'"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = Chatbot()\n",
    "chatbot(\"What is the MPAA film rating of Weathering with You?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import Config\n",
    "config = Config()\n",
    "\n",
    "DEFAULT_HOST_URL = 'https://speakeasy.ifi.uzh.ch'\n",
    "listen_freq = 2\n",
    "chatbot = Chatbot()\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        # Initialize the Speakeasy Python framework and login.\n",
    "        self.speakeasy = Speakeasy(host=config('UZH_SPEAKEASY_HOST'), username=username, password=password)\n",
    "        self.speakeasy.login()  # This framework will help you log out automatically when the program terminates.\n",
    "\n",
    "    def listen(self):\n",
    "        while True:\n",
    "            # only check active chatrooms (i.e., remaining_time > 0) if active=True.\n",
    "            rooms: List[Chatroom] = self.speakeasy.get_rooms(active=True)\n",
    "            for room in rooms:\n",
    "                if not room.initiated:\n",
    "                    # send a welcome message if room is not initiated\n",
    "                    room.post_messages(f'Hello! This is a welcome message from {room.my_alias}.')\n",
    "                    room.initiated = True\n",
    "                # Retrieve messages from this chat room.\n",
    "                # If only_partner=True, it filters out messages sent by the current bot.\n",
    "                # If only_new=True, it filters out messages that have already been marked as processed.\n",
    "                for message in room.get_messages(only_partner=True, only_new=True):\n",
    "                    print(\n",
    "                        f\"\\t- Chatroom {room.room_id} \"\n",
    "                        f\"- new message #{message.ordinal}: '{message.message}' \"\n",
    "                        f\"- {self.get_time()}\")\n",
    "\n",
    "                    # Implement your agent here #\n",
    "                    answer = chatbot(message)\n",
    "                    # Send a message to the corresponding chat room using the post_messages method of the room object.\n",
    "                    room.post_messages(f\"Received your message: '{answer}' \")\n",
    "                    # Mark the message as processed, so it will be filtered out when retrieving new messages.\n",
    "                    room.mark_as_processed(message)\n",
    "\n",
    "                # Retrieve reactions from this chat room.\n",
    "                # If only_new=True, it filters out reactions that have already been marked as processed.\n",
    "                for reaction in room.get_reactions(only_new=True):\n",
    "                    print(\n",
    "                        f\"\\t- Chatroom {room.room_id} \"\n",
    "                        f\"- new reaction #{reaction.message_ordinal}: '{reaction.type}' \"\n",
    "                        f\"- {self.get_time()}\")\n",
    "\n",
    "                    # Implement your agent here #\n",
    "\n",
    "                    room.post_messages(f\"Received your reaction: '{reaction.type}' \")\n",
    "                    room.mark_as_processed(reaction)\n",
    "\n",
    "            time.sleep(listen_freq)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time():\n",
    "        return time.strftime(\"%H:%M:%S, %d-%m-%Y\", time.localtime())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_bot = Agent(config(\"UZH_BOT_USERNAME\"), config(\"UZH_BOT_PASSWORD\"))\n",
    "    demo_bot.listen()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNC2BzrVbq+mNDgGNziLFpm",
   "collapsed_sections": [],
   "name": "dataset_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
