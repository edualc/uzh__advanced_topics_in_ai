{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53nBQpYb37bw"
   },
   "source": [
    "## 1. Data Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 12393,
     "status": "ok",
     "timestamp": 1632683772579,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "qsEynPSV4FKf",
    "outputId": "b0a602e6-abbd-497a-ad9d-abbac38a65ab"
   },
   "outputs": [],
   "source": [
    "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
    "from rdflib.term import URIRef, Literal\n",
    "import csv\n",
    "import rdflib\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'jupyterlab+svg'\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from speakeasypy import Speakeasy, Chatroom\n",
    "from typing import List\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "from thefuzz import fuzz,process\n",
    "import editdistance\n",
    "import itertools\n",
    "\n",
    "\n",
    "import jsonpickle\n",
    "# NOTE: You might have to download a few things for nltk to work properly\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import Tree\n",
    "nltk.data.path.append('/Users/melihserin/Desktop/ATAI/env/lib/python3.12/site-packages/nltk_data')\n",
    "\n",
    "# NOTE: You might have to download the en_core_web_sm model for this to work\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139148,
     "status": "ok",
     "timestamp": 1632681991550,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "lJqbjO9D4TcN",
    "outputId": "49fe0321-0ef5-4324-d56a-8aba54913547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N71b0514fbf1a459e8d7cd1366a320e86 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse('/Users/melihserin/Desktop/ATAI/dataset/14_graph.nt', format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embeddings\n",
    "entity_emb = np.load('/Users/melihserin/Desktop/ATAI/dataset/ddis-graph-embeddings/entity_embeds.npy')\n",
    "relation_emb = np.load('/Users/melihserin/Desktop/ATAI/dataset/ddis-graph-embeddings/relation_embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionaries\n",
    "with open('/Users/melihserin/Desktop/ATAI/dataset/ddis-graph-embeddings/entity_ids.del', 'r') as ifile:\n",
    "    ent2id = {str(rdflib.term.URIRef(ent)): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2ent = {v: k for k, v in ent2id.items()}\n",
    "with open('/Users/melihserin/Desktop/ATAI/dataset/ddis-graph-embeddings/relation_ids.del', 'r') as ifile:\n",
    "    rel2id = {str(rdflib.term.URIRef(rel)): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2rel = {v: k for k, v in rel2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2lbl = {str(ent): str(lbl) for ent, lbl in g.subject_objects(RDFS.label)}\n",
    "lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1632682133729,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "7KQagm9qcEFN"
   },
   "outputs": [],
   "source": [
    "# prefixes used in the graph\n",
    "WD = Namespace('http://www.wikidata.org/entity/')\n",
    "WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
    "SCHEMA = Namespace('http://schema.org/')\n",
    "DDIS = Namespace('http://ddis.ch/atai/')\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(g):\n",
    "    nodes = {}\n",
    "    query =\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "    SELECT ?lbl WHERE {{\n",
    "        <{}> rdfs:label ?lbl .\n",
    "        FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    graph_entities = set(g.subjects(unique=True)) | {s for s in g.objects(unique=True) if isinstance(s, URIRef)}\n",
    "    for node in graph_entities:\n",
    "        entity = node.toPython()\n",
    "        if isinstance(node, URIRef):            \n",
    "            qres = g.query(query.format(entity))\n",
    "            for row in qres:\n",
    "                answer = row.lbl\n",
    "            \n",
    "            nodes[str(answer)] = entity\n",
    "    return nodes\n",
    "\n",
    "def extract_predicates(g):\n",
    "    query =\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "    SELECT ?lbl WHERE {{\n",
    "        <{}> rdfs:label ?lbl .\n",
    "        FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    predicates = {}\n",
    "\n",
    "    graph_predicates = set(g.predicates(unique=True))\n",
    "    for predicate in graph_predicates:\n",
    "        predicate_ = predicate.toPython()       \n",
    "        qres = g.query(query.format(predicate_))\n",
    "        for row in qres:\n",
    "            answer = row.lbl\n",
    "        \n",
    "        predicates[str(answer)] = predicate_\n",
    "\n",
    "    return predicates\n",
    "\n",
    "\n",
    "# make variables for the nodes and predicates path\n",
    "nodes_path = '/Users/melihserin/Desktop/ATAI/dataset/processed/nodes.json'\n",
    "predicates_path = '/Users/melihserin/Desktop/ATAI/dataset/processed/predicates.json'\n",
    "\n",
    "# check indiviudally if the files exist and if so load them\n",
    "if os.path.exists(nodes_path):\n",
    "    with open(nodes_path, 'r') as ifile:\n",
    "        nodes = jsonpickle.decode(ifile.read())\n",
    "else:\n",
    "    nodes = extract_nodes(g)\n",
    "    with open(nodes_path, 'w') as ofile:\n",
    "        ofile.write(jsonpickle.encode(nodes))\n",
    "\n",
    "if os.path.exists(predicates_path):\n",
    "    with open(predicates_path, 'r') as ifile:\n",
    "        predicates = jsonpickle.decode(ifile.read())\n",
    "else:\n",
    "    predicates = extract_predicates(g)\n",
    "    with open(predicates_path, 'w') as ofile:\n",
    "        ofile.write(jsonpickle.encode(predicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot_ner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ner = ner\n",
    "        self.factual_question_patterns = [\n",
    "            \"who is the (.*) of ENTITY\",\n",
    "            \"who was the (.*) of ENTITY\",\n",
    "            \"who was the (.*) for ENTITY\",\n",
    "            \"who was the (.*) in ENTITY\",\n",
    "            \"what is the (.*) of ENTITY\",\n",
    "            \"who (.*) ENTITY\",\n",
    "            # \"who (.*) in ENTITY\",\n",
    "            \"who wrote the (.*) of ENTITY\",\n",
    "            \"who wrote the (.*) for ENTITY\",\n",
    "            \"when was ENTITY (.*)\",\n",
    "            # \"when did ENTITY (.*)\",\n",
    "            \"where was ENTITY (.*)\",\n",
    "            \"where is ENTITY (.*)\"\n",
    "        ]\n",
    "        self.nodes = nodes\n",
    "        self.predicates = predicates\n",
    "        self.entity_emb = entity_emb\n",
    "        self.relation_emb = relation_emb\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.ent2lbl = ent2lbl\n",
    "        self.lbl2ent = lbl2ent\n",
    "        self.id2ent = id2ent\n",
    "        self.WN_NOUN = 'n'\n",
    "        self.WN_VERB = 'v'\n",
    "        self.WN_ADJECTIVE = 'a'\n",
    "        self.WN_ADJECTIVE_SATELLITE = 's'\n",
    "        self.WN_ADVERB = 'r'\n",
    "    \n",
    "    def entity_extraction(self,ner_results,example):\n",
    "        entity = \"\"\n",
    "        entity_list = []\n",
    "        reset=0\n",
    "        for entity_num in range(len(ner_results)):\n",
    "            if (ner_results[entity_num][\"word\"].find(\"#\") ==-1) & (reset!=0):\n",
    "                entity = entity + \" \" + ner_results[entity_num][\"word\"]\n",
    "                reset +=1\n",
    "            else:\n",
    "                entity = entity + ner_results[entity_num][\"word\"].replace(\"#\",\"\")\n",
    "                reset +=1\n",
    "            \n",
    "            if (entity_num < len(ner_results)-1):\n",
    "                if (ner_results[entity_num+1][\"start\"] - ner_results[entity_num][\"end\"] > 3):\n",
    "                    entity_list.append(entity)\n",
    "                    reset=0\n",
    "                    entity = \"\"\n",
    "                    continue\n",
    "            else:\n",
    "                entity_list.append(entity)\n",
    "                reset=0\n",
    "                continue\n",
    "        given_entity_flawed = entity_list\n",
    "        for n,entity in enumerate(entity_list):\n",
    "            entity = entity.replace(\"?\",\"\").strip()\n",
    "            if len(entity.split(\" \"))>1:\n",
    "                try:\n",
    "                    first_word = entity.split(\" \")[0]\n",
    "                    last_word = entity.split(\" \")[-1]\n",
    "                    search_str = first_word + \"(.+?)\" + last_word\n",
    "                    entity_list[n] = re.search(search_str,example).group(0)\n",
    "                except:\n",
    "                    ent=\"\"\n",
    "                    for w in entity.split(\" \"):\n",
    "                        ent+= w + \" \"\n",
    "                    entity_list[n] = ent\n",
    "            else:\n",
    "                continue\n",
    "        return entity_list,given_entity_flawed\n",
    "    \n",
    "    def preprocessing_before_ner(self,question):\n",
    "        try:\n",
    "            question_new=re.sub(re.search(\"(.*?)of\",question).group(0), re.search(\"(.*?)of\",question).group(0).lower() ,question)\n",
    "        except:\n",
    "            words_question = question.split(\" \")\n",
    "            words_question[0] =words_question[0].lower()\n",
    "            question_new = \"\"\n",
    "            for word in words_question:\n",
    "                question_new += word + \" \"\n",
    "        return question_new\n",
    "    \n",
    "    def preprocessing_before_patterndetection(self,question):\n",
    "        tmp_words = [\" in \", \" for \",\" on \", \" of \"]\n",
    "        for tmp in tmp_words:\n",
    "            if tmp in question:\n",
    "                question=question.replace(tmp,\" \")\n",
    "        if \"the movie\" in question:\n",
    "            question=question.replace(\"the movie \",\"\")\n",
    "        return question\n",
    "    \n",
    "    def preprocessing(self,question):\n",
    "        tmp_words = [\" in \", \" for \",\" on \", \" of \"]\n",
    "        tmp_verbs = [\"was\",\"were\",\"is\",\"are\",\"did\",\"do\",\"does\",\"have\",\"has\"]\n",
    "        for tmp in tmp_words:\n",
    "            if tmp in question:\n",
    "                question=question.replace(tmp,\" \")\n",
    "        if \"the movie\" in question:\n",
    "            question=question.replace(\"the movie \",\"\")\n",
    "        if \"the\" in question:\n",
    "            index_the = [idx for idx,word in enumerate(question.split(\" \")) if word==\"the\"]\n",
    "            words_between = \"\"\n",
    "            for i in range(1,index_the[0]):\n",
    "                words_between += question.split(\" \")[i] + \" \"\n",
    "            question=question.replace(words_between.strip(),\"...\")\n",
    "        \n",
    "        if \"when\" in question.lower():\n",
    "            if question.split(\" \")[1] in tmp_verbs:\n",
    "                question=question.replace(question.split(\" \")[1],\"...\")\n",
    "        return question.replace(\"?\",\"\").lower()\n",
    "\n",
    "    # which pattern is used in the given question?\n",
    "    def pattern_detection(self,ner_results,example):\n",
    "        entities_extracted,given_entity_flawed = self.entity_extraction(ner_results,example)\n",
    "        matched_entity,_= self.match_things(self.nodes, entities_extracted[0])\n",
    "        pattern_and_entity = [[re.sub(\"ENTITY\",matched_entity, pattern),matched_entity] for pattern in self.factual_question_patterns]\n",
    "        example_updated = re.sub(given_entity_flawed[0].replace(\"?\",\"\").strip(),matched_entity, example)\n",
    "        pattern_entity_included = [lists[0] for lists in pattern_and_entity]\n",
    "        entity_from_pattern_and_entity = list(dict.fromkeys([lists[1] for lists in pattern_and_entity]))\n",
    "\n",
    "\n",
    "        question_pattern = process.extract(self.preprocessing_before_patterndetection(example_updated),pattern_entity_included,scorer=fuzz.ratio)[0][0]\n",
    "        question_pattern_ = [re.sub(value,\"ENTITY\",question_pattern) for value in entity_from_pattern_and_entity if question_pattern.find(value)!=-1][0]\n",
    "\n",
    "        index = [num for num,value in enumerate(self.factual_question_patterns) if value==question_pattern_][0]\n",
    "        # _=None\n",
    "        # if \"the\" in self.preprocessing_before_patterndetection(example_updated):\n",
    "        #     tmp=self.preprocessing_before_patterndetection(example_updated)\n",
    "        #     tmp_verbs = [\"was\",\"were\",\"is\",\"are\",\"did\",\"do\",\"does\",\"have\",\"has\"]\n",
    "        #     index_the = [idx for idx,word in enumerate(tmp.split(\" \")) if word==\"the\"]\n",
    "        #     patterns_special = []\n",
    "        #     if (tmp.split(\" \")[index_the[0]-1] not in tmp_verbs) and (index_the[0]==2):\n",
    "        #         patterns_special.append(tmp.split(\" \")[0] + \" (.*) \" + tmp.split(\" \")[index_the[0]:])\n",
    "        #         patterns_special.append(tmp.split(\" \")[:index_the[0]+1] + \" (.*) \" + tmp.split(\" \")[index_the[0]+2:])\n",
    "        #     _=patterns_special\n",
    "        return question_pattern,index,example_updated\n",
    "\n",
    "    def relation_extraction(self,ner_results,example):\n",
    "        question_pattern, index,example_updated = self.pattern_detection(ner_results,example)\n",
    "        # rels=None\n",
    "        # if _ is not None:\n",
    "        #     special_relations = []\n",
    "        #     special_relations.append(re.match(self.preprocessing(_[0]), self.preprocessing(example_updated)).group(1))\n",
    "        #     special_relations.append(re.match(self.preprocessing(_[1]), self.preprocessing(example_updated)).group(1))\n",
    "        #     relations,rels=[],[]\n",
    "        #     for rel in special_relations:\n",
    "        #         if len(rel.split(\" \"))==1 and (wn.synsets(rel)[0].pos() == self.WN_VERB):\n",
    "        #             relations.append([synonym for synonym,score in self.convert(rel, self.WN_VERB,self.WN_NOUN)])\n",
    "        #             if \"play\" in rel or \"star\" in rel:\n",
    "        #                 relations.append(\"cast member\")\n",
    "        #             rels.append(self.match_relations(relations,rel,ner_results,example)  )\n",
    "        #         else:\n",
    "        #             if (\"when\" in example.lower()) and (\"premiere\" in relation):\n",
    "        #                 rels.append(\"publication date\")\n",
    "        #             else:\n",
    "        #                 rels.append(rel)             \n",
    "\n",
    "        relation = re.match(self.preprocessing(question_pattern), self.preprocessing(example_updated)).group(1)\n",
    "        if len(relation.split(\" \"))==1 and (wn.synsets(relation)[0].pos() == self.WN_VERB):\n",
    "            relations = [synonym for synonym,score in self.convert(relation, self.WN_VERB,self.WN_NOUN)]\n",
    "            if \"play\" in relation or \"star\" in relation:\n",
    "                relations.append(\"cast member\")\n",
    "            return self.match_relations(relations,relation,ner_results,example)\n",
    "        else:\n",
    "            if (\"when\" in example.lower()) and (\"premiere\" in relation):\n",
    "                relation = \"publication date\"\n",
    "            return relation # take care of directed, released, etc. cases\n",
    "    \n",
    "    def convert(self,input, from_pos, to_pos):    \n",
    "        \"\"\" Transform words given from/to POS tags \"\"\"\n",
    "        words,temp_word_list=[],[]\n",
    "        for index,word in enumerate(input.split(\" \")):\n",
    "            synsets = wn.synsets(word, pos=from_pos)\n",
    "\n",
    "            # Word not found\n",
    "            if not synsets:\n",
    "                if len(words)==0:\n",
    "                    words.append((word,1.0))\n",
    "                else:\n",
    "                    words =[(w+\" \"+word, p) for w,p in words]\n",
    "            else:\n",
    "                # Get all lemmas of the word (consider 'a'and 's' equivalent)\n",
    "                lemmas = []\n",
    "                for s in synsets:\n",
    "                    for l in s.lemmas():\n",
    "                        if s.name().split('.')[1] == from_pos or from_pos in (self.WN_ADJECTIVE, self.WN_ADJECTIVE_SATELLITE) and s.name().split('.')[1] in (self.WN_ADJECTIVE, self.WN_ADJECTIVE_SATELLITE):\n",
    "                            lemmas += [l]\n",
    "\n",
    "                # Get related forms\n",
    "                derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
    "                # filter only the desired pos (consider 'a' and 's' equivalent)\n",
    "                related_noun_lemmas = []\n",
    "\n",
    "                for drf in derivationally_related_forms:\n",
    "                    if from_pos == \"n\":\n",
    "                        related_noun_lemmas += [drf[0]]\n",
    "                    else:\n",
    "                        for l in drf[1]:\n",
    "                            if l.synset().name().split('.')[1] == to_pos or to_pos in (self.WN_ADJECTIVE, self.WN_ADJECTIVE_SATELLITE) and l.synset().name().split('.')[1] in (self.WN_ADJECTIVE, self.WN_ADJECTIVE_SATELLITE):\n",
    "                                related_noun_lemmas += [l]\n",
    "\n",
    "                # Extract the words from the lemmas\n",
    "                temp_word_list=[l.name() for l in related_noun_lemmas]\n",
    "                temp_word_list = [(w, float(temp_word_list.count(w)) / len(temp_word_list)) for w in set(temp_word_list)]\n",
    "\n",
    "                # Take all the combinations for synonyms of different words\n",
    "                # Build the result in the form of a list containing tuples (word, probability)\n",
    "                if len(words)==0:\n",
    "                    words=temp_word_list\n",
    "                else:\n",
    "                    words =[(w_b+\" \"+w_t, p_b*p_t) for w_b,p_b in words for w_t,p_t in temp_word_list]\n",
    "                    words.sort(key=lambda w:-w[1])\n",
    "\n",
    "        # return all the possibilities sorted by probability\n",
    "        return words    \n",
    "    \n",
    "    def match_things(self,dict, input,entity_predicates=None):\n",
    "        tmp = 9999\n",
    "        match_key = \"\"\n",
    "        match_value = \"\"\n",
    "        for key, value in dict.items():\n",
    "            if editdistance.eval(key.lower(), input) < tmp:\n",
    "                tmp = editdistance.eval(key.lower(), input)\n",
    "                match_key = key\n",
    "                match_value = value\n",
    "        \n",
    "        if entity_predicates is not None:\n",
    "            tmpp = np.inf\n",
    "            match_relation_key = \"\"\n",
    "            match_relation_value= \"\"\n",
    "            for key in entity_predicates:\n",
    "                if editdistance.eval(key.lower(), input) < tmpp:\n",
    "                    tmpp = editdistance.eval(key.lower(), input)\n",
    "                    match_relation_key = key\n",
    "                    match_relation_value = self.predicates[key]\n",
    "            if editdistance.eval(match_key, match_relation_key)<=0.1*(len(match_key)+len(match_relation_key)):\n",
    "                match_key = match_relation_key\n",
    "                match_value = match_relation_value\n",
    "\n",
    "        return match_key,match_value\n",
    "\n",
    "    def match_relations(self, inputs,relation,ner_results,example):\n",
    "        tmp = 9999\n",
    "        entities,_ = self.entity_extraction(ner_results,example)\n",
    "        matched_entity, matched_entity_url= self.match_things(self.nodes, entities[0])\n",
    "        entity_predicates = list( dict.fromkeys([k for s,p,o in g.triples((URIRef(matched_entity_url), None, None)) for k,v in predicates.items() if v==str(p)]) )\n",
    "\n",
    "        match_key = \"\"\n",
    "        for input in inputs:\n",
    "            if input in entity_predicates:\n",
    "                match_key=input\n",
    "                break\n",
    "            if editdistance.eval(relation.lower(), input) < tmp:\n",
    "                tmp = editdistance.eval(relation.lower(), input)\n",
    "                match_key = input\n",
    "        return match_key\n",
    "        \n",
    "    def final_query(self,matched_entity,matched_entity_url,matched_predicate,matched_predicate_url):\n",
    "        query_option1 =\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "        SELECT ?lbl WHERE {{\n",
    "            <{}> <{}> ?answer.\n",
    "            ?answer rdfs:label ?lbl .\n",
    "            FILTER(LANG(?lbl) = \"en\").\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\".format(matched_entity_url,matched_predicate_url)\n",
    "\n",
    "        query_option2 =\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "        SELECT ?lbl WHERE {{\n",
    "            ?answer <{}> <{}>.\n",
    "            ?answer rdfs:label ?lbl .\n",
    "            FILTER(LANG(?lbl) = \"en\").\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\".format(matched_predicate_url,matched_entity_url)\n",
    "\n",
    "        qres1 = g.query(query_option1)\n",
    "        qres2 = g.query(query_option2)\n",
    "\n",
    "        answer = \"\"\n",
    "        try:\n",
    "            for row in qres1:\n",
    "                answer = row.lbl\n",
    "        except answer == \"\":\n",
    "            for row in qres2:\n",
    "                answer = row.lbl \n",
    "\n",
    "\n",
    "        if answer == \"\":\n",
    "            try:\n",
    "                answer1, answer2, answer3 = self.final_embed(matched_entity_url,matched_predicate_url)    \n",
    "                return f\"According to the embeddings, the {matched_predicate} of {matched_entity} is {answer1}, {answer2}, {answer3}.\" \n",
    "            except:  \n",
    "                return \"Sorry, I could not find the answer. Can you please rephrase the question?\"\n",
    "        else:\n",
    "            # answer1, answer2, answer3 = self.final_embed(matched_entity_url,matched_predicate_url)    \n",
    "            return f\"\"\"According to the the graph, the {matched_predicate} of {matched_entity} is {answer}.\"\"\"\n",
    "    \n",
    "    def final_embed(self,matched_entity_url,matched_predicate_url):\n",
    "        head = self.entity_emb[self.ent2id[matched_entity_url]]\n",
    "        pred = self.relation_emb[self.rel2id[matched_predicate_url]]\n",
    "        # add vectors according to TransE scoring function.\n",
    "        lhs = head + pred\n",
    "        # compute distance to *any* entity\n",
    "        dist = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        # find most plausible entities\n",
    "        most_likely = dist.argsort()\n",
    "        # compute ranks of entities\n",
    "        ranks = dist.argsort().argsort()\n",
    "\n",
    "        most_plausible_3_answers = [(str(self.id2ent[idx]), self.ent2lbl[self.id2ent[idx]])\n",
    "            for rank, idx in enumerate(most_likely[:3])]\n",
    "        \n",
    "        answer1, answer2, answer3 = most_plausible_3_answers[0][1],most_plausible_3_answers[1][1],most_plausible_3_answers[2][1]\n",
    "        return answer1, answer2, answer3\n",
    "\n",
    "    def forward(self,input):\n",
    "        ner_results = self.ner(self.preprocessing_before_ner(input))\n",
    "        entities,_ = self.entity_extraction(ner_results,input)\n",
    "        entity = entities[0]\n",
    "        relation = self.relation_extraction(ner_results,input)\n",
    "        matched_entity, matched_entity_url= self.match_things(self.nodes, entity)\n",
    "        entity_predicates = list( dict.fromkeys([k for s,p,o in g.triples((URIRef(self.nodes[matched_entity]), None, None)) for k,v in self.predicates.items() if v==str(p)]) )\n",
    "        matched_predicate, matched_predicate_url= self.match_things(self.predicates,relation,entity_predicates)\n",
    "\n",
    "        # if _ is not None:\n",
    "        #     matched_predicate1, matched_predicate_url1= self.match_things(self.predicates,_[0],entity_predicates)\n",
    "        #     matched_predicate2, matched_predicate_url2= self.match_things(self.predicates,_[1],entity_predicates)\n",
    "        #     if matched_predicate1 in entity_predicates:\n",
    "        #         matched_predicate = matched_predicate1\n",
    "        #         matched_predicate_url = matched_predicate_url1\n",
    "        #     elif matched_predicate2 in entity_predicates:\n",
    "        #         matched_predicate = matched_predicate2\n",
    "        #         matched_predicate_url=matched_predicate_url2\n",
    "        output = self.final_query(matched_entity,matched_entity_url,matched_predicate,matched_predicate_url)\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the the graph, the director of Titanic is Herbert Selpin.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = Chatbot_ner()\n",
    "chatbot('Who was the director of Titanic?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "DEFAULT_HOST_URL = config(\"UZH_SPEAKEASY_HOST\")\n",
    "listen_freq = 2\n",
    "chatbot = Chatbot_ner()\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        # Initialize the Speakeasy Python framework and login.\n",
    "        self.speakeasy = Speakeasy(host=DEFAULT_HOST_URL, username=username, password=password)\n",
    "        self.speakeasy.login()  # This framework will help you log out automatically when the program terminates.\n",
    "\n",
    "    def listen(self):\n",
    "        while True:\n",
    "            # only check active chatrooms (i.e., remaining_time > 0) if active=True.\n",
    "            rooms: List[Chatroom] = self.speakeasy.get_rooms(active=True)\n",
    "            for room in rooms:\n",
    "                if not room.initiated:\n",
    "                    # send a welcome message if room is not initiated\n",
    "                    room.post_messages(f'Hello! And Gruetzig, Merhaba, Bonjour! How can I help you today?')\n",
    "                    room.initiated = True\n",
    "                # Retrieve messages from this chat room.\n",
    "                # If only_partner=True, it filters out messages sent by the current bot.\n",
    "                # If only_new=True, it filters out messages that have already been marked as processed.\n",
    "                for message in room.get_messages(only_partner=True, only_new=True):\n",
    "                    print(\n",
    "                        f\"\\t- Chatroom {room.room_id} \"\n",
    "                        f\"- new message #{message.ordinal}: '{message.message}' \"\n",
    "                        f\"- {self.get_time()}\")\n",
    "\n",
    "                    # Implement your agent here #\n",
    "                    if (message.message.lower() == \"hi\") | (message.message.lower() == \"hello\"):\n",
    "                        answer='Hello! And Gruetzig, Merhaba, Bonjour! How can I help you today?'\n",
    "                    else:\n",
    "                        try:\n",
    "                            answer = chatbot(message.message)\n",
    "                        except:\n",
    "                            answer = \"Sorry :( I could not understand you. Can you rephrase your question?\"\n",
    "                    # Send a message to the corresponding chat room using the post_messages method of the room object.\n",
    "                    room.post_messages(f\"{answer}\")\n",
    "                    # Mark the message as processed, so it will be filtered out when retrieving new messages.\n",
    "                    room.mark_as_processed(message)\n",
    "\n",
    "                # Retrieve reactions from this chat room.\n",
    "                # If only_new=True, it filters out reactions that have already been marked as processed.\n",
    "                for reaction in room.get_reactions(only_new=True):\n",
    "                    print(\n",
    "                        f\"\\t- Chatroom {room.room_id} \"\n",
    "                        f\"- new reaction #{reaction.message_ordinal}: '{reaction.type}' \"\n",
    "                        f\"- {self.get_time()}\")\n",
    "\n",
    "                    # Implement your agent here #\n",
    "\n",
    "                    room.post_messages(f\"Received your reaction: '{reaction.type}' \")\n",
    "                    room.mark_as_processed(reaction)\n",
    "\n",
    "            time.sleep(listen_freq)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time():\n",
    "        return time.strftime(\"%H:%M:%S, %d-%m-%Y\", time.localtime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_bot = Agent(config(\"UZH_BOT_USERNAME\"), config(\"UZH_BOT_PASSWORD\"))\n",
    "demo_bot.listen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk / spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    \"What is the genre of Good Neighbors?\",\n",
    "    'Who directed Apocalypse Now?',\n",
    "    \"Who is the director of Star Wars Episode VI - Return of the Jedi?\",\n",
    "    \"Who is the screenwriter of The Masked Gang: Cyprus?\",\n",
    "    'When was The Godfather released?',\n",
    "    \"Who is the producer of Inception?\",\n",
    "    \"Who composed the soundtrack for Jurassic Park?\",\n",
    "    \"When was Pulp Fiction released?\",\n",
    "    \"Who played the lead role in The Matrix?\",\n",
    "    \"Who directed Blade Runner 2049?\",\n",
    "    \"What is the running time of The Shawshank Redemption?\",\n",
    "    \"Who was the cinematographer for Mad Max: Fury Road?\",\n",
    "    \"When did Titanic premiere?\",\n",
    "    \"Who wrote the screenplay for The Social Network?\",\n",
    "    \"What is the box office gross of Avatar?\",\n",
    "    \"Who edited the movie Parasite?\",\n",
    "    \"What is the budget of Halloween?\",\n",
    "    \"Who starred as the main character in Forrest Gump?\",\n",
    "    \"When was Interstellar first released?\",\n",
    "    \"Who is the production designer of Dune (2021)?\",\n",
    "    \"Who is the production designer of Dune?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(question):\n",
    "    doc = nlp(question)\n",
    "    displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_entity_phrase__rec(entity, preps_to_split, entities=list()):\n",
    "    if entity not in entities:\n",
    "        entities.append(entity)\n",
    "    for child in entity.children:\n",
    "        if child.dep_ == 'prep' and child in preps_to_split:\n",
    "            continue\n",
    "        entities = build_entity_phrase__rec(child, preps_to_split, entities)\n",
    "    return entities\n",
    "\n",
    "def build_entity_phrase(entity, preps_to_split):\n",
    "    entities = build_entity_phrase__rec(entity, preps_to_split)\n",
    "\n",
    "    tmp = []\n",
    "    # print(list(entity.subtree))\n",
    "    for token in entity.subtree:\n",
    "        if token in entities:\n",
    "            tmp.append(token)\n",
    "    \n",
    "    if tmp[0].text == 'the':\n",
    "        tmp = tmp[1:]\n",
    "\n",
    "    return ' '.join([token.text for token in tmp]).replace(' :', ':')\n",
    "\n",
    "def check_for_child_prep_pobj(child, entity_nodes, preps_to_split):\n",
    "    for subchild in child.children:\n",
    "        if subchild.dep_ == 'prep':\n",
    "            preps_to_split.append(subchild)\n",
    "\n",
    "            for subsubchild in subchild.children:\n",
    "                if subsubchild.dep_ == 'pobj':\n",
    "                    entity_nodes.append(subsubchild)\n",
    "                    entity_nodes, preps_to_split = check_for_child_prep_pobj(subsubchild, entity_nodes, preps_to_split)\n",
    "    return entity_nodes, preps_to_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_question(question):\n",
    "    doc = nlp(question)\n",
    "    sent = list(doc.sents)[0]\n",
    "\n",
    "    root_type = sent.root.pos_\n",
    "    # print(f\"Root Type: {root_type}\")\n",
    "\n",
    "    entity_nodes = []\n",
    "    preps_to_split = []\n",
    "\n",
    "\n",
    "    if root_type == 'AUX':\n",
    "        for child in sent.root.children:\n",
    "            if child.dep_ == 'nsubj':\n",
    "                entity_nodes.append(child)\n",
    "                entity_nodes, preps_to_split = check_for_child_prep_pobj(child, entity_nodes, preps_to_split)\n",
    "\n",
    "\n",
    "    elif root_type == 'VERB':\n",
    "        for child in sent.root.children:\n",
    "            if child.dep_ == 'dobj':\n",
    "                entity_nodes.append(child)\n",
    "                entity_nodes, preps_to_split = check_for_child_prep_pobj(child, entity_nodes, preps_to_split)\n",
    "\n",
    "            elif child.dep_ == 'prep':\n",
    "                preps_to_split.append(child)\n",
    "\n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ == 'pobj':\n",
    "                        entity_nodes.append(subchild)\n",
    "                        entity_nodes, preps_to_split = check_for_child_prep_pobj(subchild, entity_nodes, preps_to_split)\n",
    "\n",
    "            elif child.dep_ == 'nsubjpass':\n",
    "                entity_nodes.append(child)\n",
    "                entity_nodes, preps_to_split = check_for_child_prep_pobj(child, entity_nodes, preps_to_split)\n",
    "            \n",
    "            elif child.dep_ == 'nsubj':\n",
    "                if child.pos_ != 'PRON':\n",
    "                    entity_nodes.append(child)\n",
    "                    entity_nodes, preps_to_split = check_for_child_prep_pobj(child, entity_nodes, preps_to_split)\n",
    "\n",
    "\n",
    "    # print(entity_nodes)\n",
    "    entities = dict()\n",
    "    if root_type == 'VERB':\n",
    "        entities[sent.root.text] = { 'type': 'VERB', 'matches': [] }\n",
    "    for node in entity_nodes:\n",
    "        phrase = build_entity_phrase(node, preps_to_split)\n",
    "        entities[phrase] = { 'type': None, 'matches': [] }\n",
    "            \n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "WN_NOUN = 'n'\n",
    "WN_VERB = 'v'\n",
    "WN_ADJECTIVE = 'a'\n",
    "WN_ADJECTIVE_SATELLITE = 's'\n",
    "WN_ADVERB = 'r'\n",
    "\n",
    "\n",
    "def convert(input, from_pos, to_pos):    \n",
    "    \"\"\" Transform words given from/to POS tags \"\"\"\n",
    "    words,temp_word_list=[],[]\n",
    "    for index,word in enumerate(input.split(\" \")):\n",
    "        synsets = wn.synsets(word, pos=from_pos)\n",
    "\n",
    "        # Word not found\n",
    "        if not synsets:\n",
    "            if len(words)==0:\n",
    "                words.append((word,1.0))\n",
    "            else:\n",
    "                words =[(w+\" \"+word, p) for w,p in words]\n",
    "        else:\n",
    "            # Get all lemmas of the word (consider 'a'and 's' equivalent)\n",
    "            lemmas = []\n",
    "            for s in synsets:\n",
    "                for l in s.lemmas():\n",
    "                    if s.name().split('.')[1] == from_pos or from_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and s.name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "                        lemmas += [l]\n",
    "\n",
    "            # Get related forms\n",
    "            derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
    "            # filter only the desired pos (consider 'a' and 's' equivalent)\n",
    "            related_noun_lemmas = []\n",
    "\n",
    "            for drf in derivationally_related_forms:\n",
    "                if from_pos == \"n\":\n",
    "                    related_noun_lemmas += [drf[0]]\n",
    "                else:\n",
    "                    for l in drf[1]:\n",
    "                        if l.synset().name().split('.')[1] == to_pos or to_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and l.synset().name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "                            related_noun_lemmas += [l]\n",
    "\n",
    "            # Extract the words from the lemmas\n",
    "            temp_word_list=[l.name() for l in related_noun_lemmas]\n",
    "            temp_word_list = [(w, float(temp_word_list.count(w)) / len(temp_word_list)) for w in set(temp_word_list)]\n",
    "\n",
    "            # Take all the combinations for synonyms of different words\n",
    "            # Build the result in the form of a list containing tuples (word, probability)\n",
    "            if len(words)==0:\n",
    "                words=temp_word_list\n",
    "            else:\n",
    "                words =[(w_b+\" \"+w_t, p_b*p_t) for w_b,p_b in words for w_t,p_t in temp_word_list]\n",
    "                words.sort(key=lambda w:-w[1])\n",
    "\n",
    "    # return all the possibilities sorted by probability\n",
    "    return words\n",
    "\n",
    "# sorted(convert('played', WN_VERB, WN_NOUN), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_query(item_iri):\n",
    "    return \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "    SELECT ?lbl WHERE {{\n",
    "        <{}> rdfs:label ?lbl .\n",
    "        FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\".format(item_iri)\n",
    "\n",
    "def who_query(item_iri, predicate_iri):\n",
    "    return \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "    SELECT ?query WHERE {{\n",
    "        <{}> <{}> ?person .\n",
    "        ?person rdfs:label ?query .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\".format(item_iri, predicate_iri)\n",
    "\n",
    "def when_query(item_iri, predicate_iri):\n",
    "    return \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "    SELECT ?query WHERE {{\n",
    "        <{}> <{}> ?query .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\".format(item_iri, predicate_iri)\n",
    "\n",
    "def what_query(item_iri, predicate_iri):\n",
    "    return \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "    SELECT ?query WHERE {{\n",
    "        <{}> <{}> ?query .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\".format(item_iri, predicate_iri)\n",
    "\n",
    "def what_query__with_label(item_iri, predicate_iri):\n",
    "    return \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "    SELECT ?query WHERE {{\n",
    "        <{}> <{}> ?item .\n",
    "        ?item rdfs:label ?query .\n",
    "        FILTER(LANG(?query) = \"en\").\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\".format(item_iri, predicate_iri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_factual(question):\n",
    "    if 'who' in question.lower().split(' '):\n",
    "        question_type = 'who'\n",
    "    elif 'when' in question.lower().split(' '):\n",
    "        question_type = 'when'\n",
    "    elif 'what' in question.lower().split(' '):\n",
    "        question_type = 'what'\n",
    "    else:\n",
    "        question_type = 'unknown'\n",
    "\n",
    "    # get all possible entities from parsing the question\n",
    "    # ============================================================\n",
    "    #\n",
    "    parsed_dict = parse_question(question)\n",
    "    # print(parsed_dict)\n",
    "\n",
    "    # look up any possible match in the predicates/nodes\n",
    "    # ============================================================\n",
    "    #\n",
    "    def lookup_item(label, nodes, predicates):\n",
    "        matches = []\n",
    "        if label in nodes.keys():\n",
    "            matches.append(nodes[label])\n",
    "        if label in predicates.keys():\n",
    "            matches.append(predicates[label])\n",
    "        return matches\n",
    "\n",
    "    for entity in parsed_dict.keys():\n",
    "        # in case of verbs, we want to check for synonyms, e.g.\n",
    "        # \"played\" -> \"actor\"\n",
    "        #\n",
    "        if parsed_dict[entity]['type'] == 'VERB':\n",
    "            # check synonyms based on noun form of the verb\n",
    "            noun_forms = convert(entity, WN_VERB, WN_NOUN)\n",
    "\n",
    "            if question_type == 'when':\n",
    "                noun_forms.extend([(f\"{noun[0]} date\",0) for noun in noun_forms])\n",
    "                # print(noun_forms)\n",
    "\n",
    "            candidate_synonyms = list(filter(lambda x: x in nodes.keys(), [x[0] for x in noun_forms]))\n",
    "            candidate_synonyms.append(entity)\n",
    "\n",
    "            tmp = []\n",
    "            for candidate in candidate_synonyms:\n",
    "                if candidate == 'star':\n",
    "                    tmp.extend(lookup_item('cast member', nodes, predicates))\n",
    "                tmp.extend(lookup_item(candidate, nodes, predicates))\n",
    "                \n",
    "            parsed_dict[entity]['matches'] = tmp\n",
    "\n",
    "        else:\n",
    "            parsed_dict[entity]['matches'].extend(lookup_item(entity, nodes, predicates))\n",
    "\n",
    "        parsed_dict[entity]['matches'] = list(set(parsed_dict[entity]['matches']))\n",
    "    # print(parsed_dict)\n",
    "\n",
    "    # build query based on question word\n",
    "    # ============================================================\n",
    "    #\n",
    "    possible_predicates = set()\n",
    "    possible_items = set()\n",
    "\n",
    "    for phrase in parsed_dict.keys():\n",
    "        for match in parsed_dict[phrase]['matches']:\n",
    "            identifier = match.split('/')[-1]\n",
    "\n",
    "            if identifier.startswith('P'):\n",
    "                possible_predicates.add(f\"http://www.wikidata.org/prop/direct/{identifier}\")\n",
    "            elif identifier.startswith('Q'):\n",
    "                possible_items.add(f\"http://www.wikidata.org/entity/{identifier}\")\n",
    "    # print(f\"Identified Items: {possible_items}\")\n",
    "    # print(f\"Identified Predicates: {possible_predicates}\")\n",
    "    \n",
    "    # Build possible queries\n",
    "    # ============================================================\n",
    "    #\n",
    "    if question_type == 'who':\n",
    "        queries = []\n",
    "        for item in possible_items:\n",
    "            for predicate in possible_predicates:\n",
    "                queries.append(who_query(item, predicate))\n",
    "\n",
    "    elif question_type == 'when':\n",
    "        queries = []\n",
    "        for item in possible_items:\n",
    "            for predicate in possible_predicates:\n",
    "                queries.append(when_query(item, predicate))\n",
    "\n",
    "    elif question_type == 'what':\n",
    "        queries = []\n",
    "        for item in possible_items:\n",
    "            for predicate in possible_predicates:\n",
    "                queries.append(what_query(item, predicate))\n",
    "                queries.append(what_query__with_label(item, predicate))\n",
    "\n",
    "    # else:\n",
    "        # print('UNKNOWN QUESTION TYPE')\n",
    "\n",
    "\n",
    "    # Execute queries\n",
    "    # ============================================================\n",
    "    #\n",
    "    query_answered = False\n",
    "    for query in queries:\n",
    "        # print(query)\n",
    "        res = g.query(query)\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            continue\n",
    "\n",
    "        for row in res:\n",
    "            result = row.query\n",
    "            \n",
    "            if question_type in ['when', 'what']:\n",
    "                if not isinstance(result, Literal):\n",
    "                    continue\n",
    "\n",
    "            # print(type(result))\n",
    "            if result is not None:\n",
    "                query_answered = True\n",
    "                return f\"Answer: {result} (from Graph)\"\n",
    "\n",
    "    if not query_answered:\n",
    "        return \"Could not find answer in graph\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_model_chatbot(input):\n",
    "    output = answer_factual(input)\n",
    "    if output == \"Could not find answer in graph\":\n",
    "        try:\n",
    "            output = chatbot(input)\n",
    "        except:\n",
    "            output=\"Sorry, I could not find the answer. Can you please rephrase the question?\"\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_outputs = {}\n",
    "for question in TEST_QUESTIONS:\n",
    "    dict_outputs[question] = hybrid_model_chatbot(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What is the genre of Good Neighbors?': 'Answer: art film (from Graph)',\n",
       " 'Who directed Apocalypse Now?': 'According to the the graph, the director of Apocalypse Now is Francis Ford Coppola.',\n",
       " 'Who is the director of Star Wars Episode VI - Return of the Jedi?': 'According to the embeddings, the director of Star Wars Episode VI: Return of the Jedi is George Lucas, Anthony Daniels, Ellis Rubin.',\n",
       " 'Who is the screenwriter of The Masked Gang: Cyprus?': 'According to the embeddings, the screenwriter of The Masked Gang: Cyprus is Cengiz Küçükayvaz, Murat Aslan, Melih Ekener.',\n",
       " 'When was The Godfather released?': 'Answer: 1972-03-15 (from Graph)',\n",
       " 'Who is the producer of Inception?': 'According to the embeddings, the presenter of Inception is Satomi Ishihara, Michael Aspel, Heino Ferch.',\n",
       " 'Who composed the soundtrack for Jurassic Park?': 'According to the embeddings, the country of Jurassic Park is Jurassic Park, Jurassic World: Dominion, Jurassic Park.',\n",
       " 'When was Pulp Fiction released?': 'Answer: 1994-05-21 (from Graph)',\n",
       " 'Who played the lead role in The Matrix?': 'According to the embeddings, the lifestyle of The Matrix is The Matrix, personal identity, playboy.',\n",
       " 'Who directed Blade Runner 2049?': 'Answer: Denis Villeneuve (from Graph)',\n",
       " 'What is the running time of The Shawshank Redemption?': 'According to the embeddings, the religion of The Shawshank Redemption is religion, The Shawshank Redemption, point of view.',\n",
       " 'Who was the cinematographer for Mad Max: Fury Road?': 'According to the embeddings, the choreographer of Mad Max: Fury Road is Saroj Khan, Hermes Pan, Rod Hardy.',\n",
       " 'When did Titanic premiere?': 'According to the embeddings, the creator of Titanic is Titanic, Herbert Selpin, Kirsten Heiberg.',\n",
       " 'Who wrote the screenplay for The Social Network?': 'According to the embeddings, the screenwriter of The Social Network is Jan Rychlík, Joseph Koo, Svatopluk Havelka.',\n",
       " 'What is the box office gross of Avatar?': 'Sorry, I could not find the answer. Can you please rephrase the question?',\n",
       " 'Who edited the movie Parasite?': 'According to the embeddings, the creator of Parasite is Parasite, Bong Joon-ho, Yang Jin-mo.',\n",
       " 'What is the budget of Halloween?': 'Sorry, I could not find the answer. Can you please rephrase the question?',\n",
       " 'Who starred as the main character in Forrest Gump?': 'According to the embeddings, the characters of Forrest Gump is Forrest Gump, Jenny Curran, Joseph J. Romm.',\n",
       " 'When was Interstellar first released?': 'Answer: 2014-10-26 (from Graph)',\n",
       " 'Who is the production designer of Dune (2021)?': 'According to the embeddings, the production designer of Dune is David Lynch, Julieta Rosen, Ernesto Laguardia Longega.',\n",
       " 'Who is the production designer of Dune?': 'According to the embeddings, the production designer of Dune is David Lynch, Julieta Rosen, Ernesto Laguardia Longega.'}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = ner(\"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\")\n",
    "input= \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_extraction(ner_results,example):\n",
    "    entity = \"\"\n",
    "    entity_list = []\n",
    "    reset=0\n",
    "    for entity_num in range(len(ner_results)):\n",
    "        if (ner_results[entity_num][\"word\"].find(\"#\") ==-1) & (reset!=0):\n",
    "            entity = entity + \" \" + ner_results[entity_num][\"word\"]\n",
    "            reset +=1\n",
    "        else:\n",
    "            entity = entity + ner_results[entity_num][\"word\"].replace(\"#\",\"\")\n",
    "            reset +=1\n",
    "        \n",
    "        if (entity_num < len(ner_results)-1):\n",
    "            if (ner_results[entity_num+1][\"start\"] - ner_results[entity_num][\"end\"] > 3):\n",
    "                entity_list.append(entity)\n",
    "                reset=0\n",
    "                entity = \"\"\n",
    "                continue\n",
    "        else:\n",
    "            entity_list.append(entity)\n",
    "            reset=0\n",
    "            continue\n",
    "    for n,entity in enumerate(entity_list):\n",
    "        entity = entity.replace(\"?\",\"\").strip()\n",
    "        if len(entity.split(\" \"))>1:\n",
    "            try:\n",
    "                first_word = entity.split(\" \")[0]\n",
    "                last_word = entity.split(\" \")[-1]\n",
    "                search_str = first_word + \"(.+?)\" + last_word\n",
    "                entity_list[n] = re.search(search_str,example).group(0)\n",
    "            except:\n",
    "                ent=\"\"\n",
    "                for w in entity.split(\" \"):\n",
    "                    ent+= w + \" \"\n",
    "                entity_list[n] = ent\n",
    "        else:\n",
    "            continue\n",
    "    temp = entity_list\n",
    "    idx = []\n",
    "    for i,ent in enumerate(entity_list):\n",
    "        if \",\" in ent:\n",
    "            idx.append(i)\n",
    "            if \",and\" in ent or \", and\" in ent:\n",
    "                temp[i] =ent.replace(\",and\",\",\").replace(\", and\",\",\")\n",
    "            temp=temp + temp[i].split(\",\")\n",
    "    for i in sorted(idx,reverse=True):\n",
    "        temp.pop(i)\n",
    "    entity_list = list(dict.fromkeys([ent.strip() for ent in temp]))\n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Lion King', 'Pocahontas', 'The Beauty and the Beast']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = entity_extraction(ner_results,input)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.wikidata.org/entity/Q3110682',\n",
       " 'http://www.wikidata.org/entity/Q679670',\n",
       " 'http://www.wikidata.org/entity/Q13417189']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_entities = [nodes[ent] for ent in entities]\n",
    "urls_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_movies():\n",
    "    movies = {}\n",
    "    for ent, url in nodes.items():\n",
    "        if len([o for s,p,o in g.triples((URIRef(url), WDT.P31, WD.Q11424))])!=0:\n",
    "            movies[ent] = url\n",
    "    return movies\n",
    "\n",
    "def extract_genres():\n",
    "    genres = {}\n",
    "    for genre_url in list(dict.fromkeys([str(o) for s,p,o in g.triples((None, URIRef(predicates[\"genre\"]), None))])):\n",
    "        if str(genre_url) not in genres.values():\n",
    "            label = [node for node,url in nodes.items() if url == str(genre_url)]\n",
    "            genres[label] = str(genre_url)\n",
    "    return genres\n",
    "\n",
    "def url2nodes():\n",
    "    url2nodes_ = dict(zip(nodes.values(),nodes.keys()))\n",
    "    return ent2lbl | url2nodes_\n",
    "\n",
    "movies_path = '/Users/melihserin/Desktop/ATAI/dataset/processed/movies.json'\n",
    "url2nodes_path = '/Users/melihserin/Desktop/ATAI/dataset/processed/url2nodes.json'\n",
    "\n",
    "\n",
    "if os.path.exists(url2nodes_path):\n",
    "    with open(url2nodes_path, 'r') as ifile:\n",
    "        nodes = jsonpickle.decode(ifile.read())\n",
    "else:\n",
    "    url2nodes_ = url2nodes()\n",
    "    with open(url2nodes_path, 'w') as ofile:\n",
    "        ofile.write(jsonpickle.encode(url2nodes_))\n",
    "\n",
    "\n",
    "# check indiviudally if the files exist and if so load them\n",
    "if os.path.exists(movies_path):\n",
    "    with open(movies_path, 'r') as ifile:\n",
    "        movies = jsonpickle.decode(ifile.read())\n",
    "else:\n",
    "    movies = extract_movies()\n",
    "    with open(movies_path, 'w') as ofile:\n",
    "        ofile.write(jsonpickle.encode(movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_genres():\n",
    "    genres = {}\n",
    "    for genre_url in list(dict.fromkeys([str(o) for s,p,o in g.triples((None, URIRef(predicates[\"genre\"]), None))])):\n",
    "        if genre_url not in genres.values():\n",
    "            try:\n",
    "                label = ent2lbl[genre_url]\n",
    "                genres[label] = genre_url\n",
    "            except:\n",
    "                continue\n",
    "    return genres\n",
    "\n",
    "genres = extract_genres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_list_items(list_of_lists):\n",
    "    num_lists = len(list_of_lists)\n",
    "    matched_items = []\n",
    "    for item in list_of_lists[0]:\n",
    "        matched_num = 0\n",
    "        for i in range(1,num_lists):\n",
    "            if item in list_of_lists[i]:\n",
    "                matched_num+=1\n",
    "        if matched_num==num_lists-1:\n",
    "            matched_items.append(item)\n",
    "    return matched_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_genre_embed(matched_entity_url_list):\n",
    "    genre = relation_emb[rel2id[predicates[\"genre\"]]]\n",
    "    dist_genre = np.zeros(entity_emb.shape[0])\n",
    "    dist_movie = np.zeros(entity_emb.shape[0])\n",
    "    idx_movie = []\n",
    "    objects_movie=[]\n",
    "    # add vectors according to TransE scoring function.\n",
    "    for entity_url in matched_entity_url_list:\n",
    "        objects_movie.append(list(dict.fromkeys([str(o) for s,p,o in g.triples((URIRef(entity_url), None, None)) if str(o) in genres.values()])))\n",
    "        head = entity_emb[ent2id[entity_url]]\n",
    "        lhs = head + genre\n",
    "        # compute distance to *any* entity\n",
    "        dist_movie += pairwise_distances(head.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        idx_movie.append(pairwise_distances(head.reshape(1, -1), entity_emb).reshape(-1).argmin())\n",
    "        dist_genre += pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "    # find most plausible entities\n",
    "    matched_genres = match_list_items(objects_movie)\n",
    "    if 'http://www.wikidata.org/entity/Q11424' in matched_genres:\n",
    "        matched_genres.remove('http://www.wikidata.org/entity/Q11424')\n",
    "    most_likely_movie = dist_movie.argsort()\n",
    "    most_likely_genre = dist_genre.argsort()\n",
    "    # compute ranks of entities\n",
    "    # ranks = dist.argsort().argsort()\n",
    "    most_plausible_movies = [(str(id2ent[idx]), url2nodes_[id2ent[idx]])\n",
    "            for idx in most_likely_movie[:100] \n",
    "                    if url2nodes_[id2ent[idx]] in movies.keys()\n",
    "                                            and idx not in idx_movie \n",
    "                                                    and url2nodes_[id2ent[idx]] not in [url2nodes_[url] for url in matched_entity_url_list]]\n",
    "    \n",
    "    if len(matched_genres)!=0:\n",
    "        most_plausible_genre = [(url, url2nodes_[url])\n",
    "        for url in matched_genres]\n",
    "    else:\n",
    "        most_plausible_genre = [(str(id2ent[idx]), url2nodes_[id2ent[idx]])\n",
    "            for idx in most_likely_genre[:3]]\n",
    "    \n",
    "    # answer1= most_plausible_answer[0][1]\n",
    "    return most_plausible_movies[0], most_plausible_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('http://www.wikidata.org/entity/Q630289', 'The Thirteenth Floor'),\n",
       " [('http://www.wikidata.org/entity/Q130232', 'drama'),\n",
       "  ('http://www.wikidata.org/entity/Q157443', 'comedy film'),\n",
       "  ('http://www.wikidata.org/entity/Q157394', 'fantasy film')])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendation = recommend_genre_embed(urls_entities)\n",
    "recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_chatbot(input):\n",
    "    # try:\n",
    "        ner_results=ner(input)\n",
    "        if len(ner_results)==0:\n",
    "            genre_search = [(i,i+\" film\") for i in input.split(\" \")]\n",
    "            matched_genres = []\n",
    "            for word,word_film in genre_search:\n",
    "                if word in genres.keys():\n",
    "                    matched_genres.append((word,genres[word]))\n",
    "                if word_film in genres.keys():\n",
    "                    matched_genres.append((word_film,genres[word_film]))\n",
    "            recommendation = recommend_genre_embed([url for label,url in matched_genres])\n",
    "            genres_matched = \"\"\n",
    "            for matched in list(dict.fromkeys([label.replace(\" film\",\"\") for label,url in matched_genres])):\n",
    "                genres_matched +=matched + \", \"\n",
    "            genres_matched = genres_matched.strip()[:-1]\n",
    "            return f\"If you like {genres_matched} movies, I would recommend you watch {recommendation[0][1]}.\"\n",
    "            \n",
    "        entities=entity_extraction(ner_results,input)\n",
    "        entities_matched = [chatbot.match_things(nodes,ent) for ent in entities]\n",
    "        urls_entities = [url for ent,url in entities_matched]\n",
    "        try:\n",
    "            pub_dates = sorted(list(dict.fromkeys([[str(o)[:3] for s,p,o in g.triples((URIRef(url), URIRef(predicates[\"publication date\"]), None))][0]\n",
    "                            for url in urls_entities])))\n",
    "        except:\n",
    "            pub_dates=[]\n",
    "        recommendation = recommend_genre_embed(urls_entities)\n",
    "        genres_matched = \"\"\n",
    "        for i in range(len(recommendation[1])):\n",
    "            if recommendation[1][i][1].split(\" \")[0]==\"film\":\n",
    "                continue\n",
    "            genres_matched+=recommendation[1][i][1].replace(\" film\",\"\") +\",\"\n",
    "        genres_matched = genres_matched[:-1] +\" movies\"\n",
    "        if len(pub_dates)==2:\n",
    "            genres_matched += \" from around \" + pub_dates[0] +\"0s or \" + pub_dates[1] + \"0s\"\n",
    "        elif len(pub_dates)==1:\n",
    "            genres_matched += \" from around \" + pub_dates[0] +\"0s\"\n",
    "        return f\"Based on what you like, I would recommend you watching {genres_matched} such as {recommendation[0][1]}.\"\n",
    "    # except:\n",
    "    #     return \"Sorry, I cannot recommend you a movie based on your query. The reasons might be that I do not know the movies you mentioned or there is a minor problem with the format of your input. You might want to re-check and/or rephrase your sentence. I will be waiting here. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you like comedy movies, I would recommend you watch The Leisure Seeker.'"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendation_chatbot(\"I want to watch comedy movies.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNC2BzrVbq+mNDgGNziLFpm",
   "collapsed_sections": [],
   "name": "dataset_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
